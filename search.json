[{"title":"网关服务-3-过滤器","url":"/2022/08/28/网关服务-3-过滤器/","content":"\n每个客户端用户请求微服务应用提供的接口时，它们的访问权限往往都需要有一定的限制，系统并不会将所有的微服务接口都对它们开放。然而，目前的服务路由并没有限制权限这样的功能，所有请求都会被毫无保留地转发到具体的应用并返回结果，<!--more-->为了实现对客户端请求的安全校验和权限控制，最简单和粗暴的方法就是为每个微服务应用都实现一套用于校验签名和鉴别权限的过滤器或拦截器。不过，这样的做法并不可取，它会增加日后的系统维护难度，因为同一个系统中的各种校验逻辑很多情况下都是大致相同或类似的，这样的实现方式会使得相似的校验逻辑代码被分散到了各个微服务中去，冗余代码的出现是我们不希望看到的。所以，比较好的做法是将这些校验逻辑剥离出去，构建出一个独立的鉴权服务。在完成了剥离之后，有不少开发者会直接在微服务应用中通过调用鉴权服务来实现校验，但是这样的做法仅仅只是解决了鉴权逻辑的分离，并没有在本质上将这部分不属于业余的逻辑拆分出原有的微服务应用，冗余的拦截器或过滤器依然会存在。\n\n对于这样的问题，更好的做法是通过前置的网关服务来完成这些非业务性质的校验。由于网关服务的加入，外部客户端访问我们的系统已经有了统一入口，既然这些校验与具体业务无关，那何不在请求到达的时候就完成校验和过滤，而不是转发后再过滤而导致更长的请求延迟。同时，通过在网关中完成校验和过滤，微服务应用端就可以去除各种复杂的过滤器和拦截器了，这使得微服务应用的接口开发和测试复杂度也得到了相应的降低。\n\n\n为了在API网关中实现对客户端请求的校验，我们将需要使用到Spring Cloud Zuul的另外一个核心功能：过滤器。\n\nZuul允许开发者在API网关上通过定义过滤器来实现对请求的拦截与过滤，实现的方法非常简单，我们只需要继承ZuulFilter抽象类并实现它定义的四个抽象函数就可以完成对请求的拦截和过滤了。\n\n* 1.过滤器的实现 \n```java\n\npublic class AccessFilter extends ZuulFilter  {\n\n    private static Logger log = LoggerFactory.getLogger(AccessFilter.class);\n\n    @Override\n    public String filterType() {\n        return \"pre\";\n    }\n\n    @Override\n    public int filterOrder() {\n        return 0;\n    }\n\n    @Override\n    public boolean shouldFilter() {\n        return true;\n    }\n\n    @Override\n    public Object run() {\n        RequestContext ctx = RequestContext.getCurrentContext();\n        HttpServletRequest request = ctx.getRequest();\n\n      \tlog.info(\"send {} request to {}\", request.getMethod(), request.getRequestURL().toString());\n\n        Object accessToken = request.getParameter(\"accessToken\");\n        if(accessToken == null) {\n            log.warn(\"access token is empty\");\n            ctx.setSendZuulResponse(false);\n            ctx.setResponseStatusCode(401);\n            return null;\n        }\n        log.info(\"access token ok\");\n        return null;\n    }\n\n}\n```\n\n以上是一个过滤器的简单实现，实现了在请求被路由之前，会检测HttpServletRequest中是否有accessToken参数，对于没有这个参数的请求直接报错，拒绝进行路由。\n实现一个过滤去主要是通过继承ZuulFilter抽象类重写其中的四个方法来实现：  \n   * 1.filterType：定义当前过滤器的类型，主要是决定当前的过滤器会在那个生命周期中执行，上文中定义为pre，表示在路由之前被执行 \n   * 2.filterOrder：定义了当前过滤器执行的顺序，当在一个请求阶段中存在多个过滤器时，需要根据当前方法的返回值来按序进行执行过滤器\n   * 3.shouldFilter：判断是否需要执行当前过滤器，文中直接返回true，该过滤器会对所有的请求生效。\n   * 4.run：过滤器的具体逻辑实现\n\n在定义了过滤器之后不会自动的生效，需要我们为其创建具体的bean才能启动该过滤器，例如在主类中加入一下\n```java\n@EnableZuulProxy\n@SpringCloudApplication\npublic class Application {\n\n\tpublic static void main(String[] args) {\n\t\tnew SpringApplicationBuilder(Application.class).web(true).run(args);\n\t}\n\n\t@Bean\n\tpublic AccessFilter accessFilter() {\n\t\treturn new AccessFilter();\n\t}\n}\n```\n\n在定义了上面的bean之后该过滤器就会在当前服务中生效，过滤请求中没有token的请求路由操作。\n","tags":["gateWay"],"categories":["spring"]},{"title":"网关服务-2-路由配置","url":"/2022/08/28/网关服务-2-路由配置/","content":"* 单实例配置 \n```xml\nzuul.routes.user-service.path=/user-service/**\nzuul.routes.user-service.url=http://localhost:8080/\n```\n<!--more-->\n将所有的符合/user-servcie/**规则请求的都发送到http://localhost:8080/ 上面。\n\n* 多实例配置  \n```xml\nzuul.routes.user-service.path=/user-service/**\nzuul.routes.user-service.serviceId=user-service\n\nribobon.eureka.enabled=false\nuser-service.ribbon.listOfServices=http://localhost:8080,http://localhost:8081/\n```\n通过zuul.routes.<route>.servcieId 自定义了一个serviceId，然后是用这个serviceId来<serviceId>.ribbon.listOfServices来指定多个实例地址，对应上面的<route>指定的请求/user-service/**  \n\n注意：不论是单实例的还是双实例的，我们都需要为每一对映射关系指定一个名称也就是上面的<route>,每一个<route>就对应了一条路由规则，每调路由规则都需要通过path属性来定义匹配一个客户端请求的路径表达式，通过url或者serviceId来映射其对应的实例地址，或者服务id名。\n\n\n\n\n","tags":["gateWay"],"categories":["spring"]},{"title":"网关服务-1基础","url":"/2022/08/28/网关服务-1基础/","content":"![20211215102108](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/20211215102108.png)  \n<!--more-->\n我们使用Spring Cloud Netflix中的Eureka实现了服务注册中心以及服务注册与发现；而服务间通过Ribbon或Feign实现服务的消费以及均衡负载；通过Spring Cloud Config实现了应用多环境的外部化配置以及版本管理。为了使得服务集群更为健壮，使用Hystrix的融断机制来避免在微服务架构中个别服务出现异常时引起的故障蔓延。   \n\n当前架构的缺点 ：  \n* 首先，破坏了服务无状态特点。为了保证对外服务的安全性，我们需要实现对服务访问的权限控制，而开放服务的权限控制机制将会贯穿并污染整个开放服务的业务逻辑，这会带来的最直接问题是，破坏了服务集群中REST API无状态的特点。从具体开发和测试的角度来说，在工作中除了要考虑实际的业务逻辑之外，还需要额外可续对接口访问的控制处理。\n* 其次，无法直接复用既有接口。当我们需要对一个即有的集群内访问接口，实现外部服务访问时，我们不得不通过在原有接口上增加校验逻辑，或增加一个代理调用来实现权限控制，无法直接复用原有的接口。\n\n服务网关统一向外系统提供REST API的过程中，除了具备**服务路由**、**均衡负载**功能之外，它还具备了**权限控制**等功能。\n\n# 基础的网关服务构建 \n\n* 1.启动两个服务 \n  * eureka-client\n  * eureka-consumer  \n\n* 2.导入相关的依赖\n  ```xml\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>1.5.4.RELEASE</version>\n        <relativePath/>\n    </parent>\n\n    <dependencies>\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-zuul</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-eureka</artifactId>\n    </dependency>\n    </dependencies>\n\n    <dependencyManagement>\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-dependencies</artifactId>\n            <version>Dalston.SR1</version>\n            <type>pom</type>\n            <scope>import</scope>\n        </dependency>\n    </dependencies>\n    </dependencyManagement>\n\n  ```\n* 3.创建应用主类，开启相关的注解\n```java\n    @EnableZuulProxy\n    @SpringCloudApplication\n    public class Application {\n    \n    public static void main(String[] args) {\n        new SpringApplicationBuilder(Application.class).web(true).run(args);\n    }\n    \n    }\n```\n\n* 3.创建配置文件application.yaml，并加入服务名、端口号、eureka注册中心的地址：  \n```yml\nspring:\n  application:\n    name: api-gateway\n\nserver:\n  port: 1101\n\neureka:\n  client:\n    serviceUrl:\n      defaultZone: http://eureka.didispace.com/eureka/\n```\n\n到这里，一个基于Spring Cloud Zuul服务网关就已经构建完毕。启动该应用，一个默认的服务网关就构建完毕了。由于Spring Cloud Zuul在整合了Eureka之后，具备默认的服务路由功能，即：当我们这里构建的api-gateway应用启动并注册到eureka之后，服务网关会发现上面我们启动的两个服务eureka-client和eureka-consumer，这时候Zuul就会创建两个路由规则。每个路由规则都包含两部分，一部分是外部请求的匹配规则，另一部分是路由的服务ID。针对当前示例的情况，Zuul会创建下面的两个路由规则：\n\n转发到eureka-client服务的请求规则为：/eureka-client/**\n转发到eureka-consumer服务的请求规则为：/eureka-consumer/**\n最后，我们可以通过访问1101端口的服务网关来验证上述路由的正确性：\n\n比如访问：http://localhost:1101/eureka-client/dc ，该请求将最终被路由到eureka-client的/dc接口上。\n","tags":["gateWay"],"categories":["spring"]},{"title":"java动态代理源码阅读","url":"/2022/07/20/java动态代理源码阅读/","content":"       \n\n## 1.什么是代理和动态代理 \n代理的主要是为了不改动被代理对象的代码基础上给予其更多的功能，常用的代理实现通过被代理对象，代理类同时继承同一个类，或者同时实现同一个接口，这种有编程开发者自己编写的代理类是实现的方法为静态代理；同时在系统开发中，可能由于需要实现的代理类很多，或者在程序开发时我们无法确定程序运行时具体的代理类功能，这种情况下就不能使用静态代理的方法，需要使用动态代理。另一方动态的代理的实现可以有两种方法，一种是使用java中的动态代理，还有就是使用cglib来实现动态代理，本文中主要介绍学习java动态代理的实现以及源码的解析。  \n<!--more-->\njava动态代理的原理是基于java反射来实现的。  \n\n## 2.java动态代理的主要步骤 \n&emsp;&emsp;其实对于java实现动态代理的步骤而言是非常固定的，主要可以分成以下几个步骤：  \n* 1.创建一个被代理对象和代理对象共同实现的接口，以及被代理对象的类；   \n* 2.创建一个实现InvocationHandler接口的代理对象具体实现类，并实现其中的invoke（）方法；      \n* 3.使用Proxy类中的newProxyInstance方法，传入被代理对象的classloader和被代理对象的interface，以及实现代理类功能的增强类的对象，获得对应的动态代理对象，用其和被代理对象的接口的引用来指向具体的动态代理对象；  \n* 4.通过接口的引用来调用对应的方法，实现动态代理对象中真正的方法。   \n  \n## 3.原理图\n![proxy](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/proxy.png)   \n\n## 4.代码解析   \n&emsp;&emsp;主要看看一下具体实现代理类功能的handler接口实现类LianJiaHandler.class 和 主类中通过Proxy类获取并使用该动态代理类的使用过程。   \n### 4.1 LianJiaHandler.class\n```java\npublic class LianJiaHandler implements InvocationHandler{\n\n\n\n    /*具体实现动态代理类对象方法的功能 */\n    @Overried\n    public Object invoke(){\n        \n    }\n}\n```\n\n### 4.2 Proxy.class\n","tags":["java基础"],"categories":["java"]},{"title":"并发编程的原子性，可见性，有序性","url":"/2022/06/20/并发编程的原子性，可见性，有序性/","content":"在前面谈到了一些关于内存模型以及并发编程中可能会出现的一些问题。下面我们来看一下Java内存模型，研究一下Java内存模型为我们提供了哪些保证以及在java中提供了哪些方法和机制来让我们在进行多线程编程时能够保证程序执行的正确性。\n<!--more-->\n　　在Java虚拟机规范中试图定义一种Java内存模型（Java Memory Model，JMM）来屏蔽各个硬件平台和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。那么Java内存模型规定了哪些东西呢，它定义了程序中变量的访问规则，往大一点说是定义了程序执行的次序。注意，为了获得较好的执行性能，Java内存模型并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序。也就是说，在java内存模型中，也会存在缓存一致性问题和指令重排序的问题。\n\n　　Java内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。\n\n　　举个简单的例子：在java中，执行下面这个语句：\n\ni  = 10;\n 　　执行线程必须先在自己的工作线程中对变量i所在的缓存行进行赋值操作，然后再写入主存当中。而不是直接将数值10写入主存当中。\n\n　　那么Java语言 本身对 原子性、可见性以及有序性提供了哪些保证呢？\n\n# 1.原子性\n\n　　在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。\n\n　　上面一句话虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子i：\n\n　　请分析以下哪些操作是原子性操作：\n* 1.x = 10;         //语句1\n* 2.y = x;         //语句2\n* 3.x++;           //语句3\n* 4.x = x + 1;     //语句4\n 　　咋一看，有些朋友可能会说上面的4个语句中的操作都是原子性操作。其实只有语句1是原子性操作，其他三个语句都不是原子性操作。\n\n　　语句1是直接将数值10赋值给x，也就是说线程执行这个语句的会直接将数值10写入到工作内存中。\n\n　　语句2实际上包含2个操作，它先要去读取x的值，再将x的值写入工作内存，虽然读取x的值以及 将x的值写入工作内存 这2个操作都是原子性操作，但是合起来就不是原子性操作了。\n\n　　同样的，x++和 x = x+1包括3个操作：读取x的值，进行加1操作，写入新的值。所以上面4个语句只有语句1的操作具备原子性。\n\n　　也就是说，只有简单的读取、赋值（而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作）才是原子操作。\n\n　　不过这里有一点需要注意：在32位平台下，对64位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的JDK中，JVM已经保证对64位数据的读取和赋值也是原子性操作了。\n\n　　从上面可以看出，Java内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过synchronized和Lock来实现。由于synchronized和Lock能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。\n\n# 2.可见性\n\n　　对于可见性，Java提供了volatile关键字来保证可见性。\n\n　　当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。\n\n　　而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。\n\n　　另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。\n\n# 3.有序性\n\n　　在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。\n\n　　在Java里面，可以通过volatile关键字来保证一定的“有序性”（具体原理在下一节讲述）。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。\n\n　　另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。\n\n　　下面就来具体介绍下happens-before原则（先行发生原则）：\n\n* 1.程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作\n* 2.锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作\n* 3.volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作\n* 4.传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C\n* 5.线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作\n* 6.线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生\n* 7.线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束,Thread.isAlive()的返回值手段检测到线程已经终止执行\n* 8.对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始\n　　这8条原则摘自《深入理解Java虚拟机》。\n\n　　这8条规则中，前4条规则是比较重要的，后4条规则都是显而易见的。\n\n　　下面我们来解释一下前4条规则：\n\n　　对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。\n\n　　第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果出于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行lock操作。\n\n　　第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。\n\n　　第四条规则实际上就是体现happens-before原则具备传递性。\n","tags":["java基础"],"categories":["java"]},{"title":"集合中的泛型和多态","url":"/2022/06/20/集合中的泛型和多态/","content":"**泛型**：泛型的作用主要是用来检查数据类型是否何方，最常见的泛型使用如集合中的泛型，通过在<>设置数据类型，List<String> strList,则集合strList中存储的元素只能是String数据类型的，<!--more-->当存放String之外的且不是String的子类时，在程序编译的时候会报错。保护了程序编写的安全性。\n\n&emsp;&emsp;**多态**:多态的  \n\n现在在一个方法中定义   \n```java\npublic void method(List<Parent> list1){}\n```  \n当我们想要传入一个List<Son> list1 这个时候会进行编译报错，java里面是不允许这种操作的，\n","tags":["java基础"],"categories":["java"]},{"title":"redis作为分布式锁","url":"/2022/04/06/redis作为分布式锁/","content":"对于分布式程序而言，当多个主机中的程序需要操作同一个变量时，由于程序是分布在不同的主机之上，不在同一个jvm中，不能使用原有的锁来进行同步，这个时候就用到了分布式锁来同步。常见的分布式锁可以有三种方法来进行实现：  \n<!--more-->\n* 1.数据库实现分布式锁\n* 2.实现分布式锁\n* 3.zookeeper来实现分布式锁\n\n# 1.数据库实现分布式锁\n&emsp;&emsp;最简单的就是直接创建一张所锁表，通过操作该表中的数据来实现，当程序需要获得锁的时候就写入一条数据，想要释放锁的时候就删除这条数据。   \n&emsp;&emsp;这种锁实现很简单但是存在一些缺点：  \n* 1.这种锁没有失效时间，一旦释放锁的操作失败就会导致锁记录一直存在于锁表中，其他显示线程就无法获得这个锁，但是也可以设置定期的去清除数据来解决；  \n* 2.这种实现依赖于数据库，可靠性不佳，数据库一般是单点，一旦出问题，影响大   \n* 3.这种锁是非可重入的，因为同一个线程在没有释放锁资源的之前无法重新获取锁，因为在数据库中已经有一份记录了，想要实现可重入可以在多设计一些字段，比如获得主机信息，线程信息等，那么在再次获得锁的时候，可以先查询数据，如果当前的表可以查询到当前的主机或者线程的记录时，则直接将锁分配给它们。  \n\n\nredis作为分布式锁的8种问题，https://mp.weixin.qq.com/s/nSvBaNBYTZ_0Fk-9xkOLQg \n\n# 2.Redis 作为分布式锁 \n* 1.获取锁的时候用setnx存储对应的key到redis中，并使用expire设置过期时间，成功获取锁的时候会返回1，否则返回0\n* 2.在释放锁的时候，可以和key中的value进行比较，看是否是当前主机或者进程创建的锁，校验之后使用del对锁进行释放。\n\n\n实例demo:\n","tags":["redis"],"categories":["cache"]},{"title":"MQ消息队列（5）Kafka","url":"/2022/04/06/MQ消息队列（5）Kafka/","content":"\n![20220330164832](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/20220330164832.png)\n<!--more-->\n","tags":["kafka"],"categories":["messageQueue"]},{"title":"redis基本命令使用","url":"/2022/03/03/redis基本命令使用/","content":"\nREmote DIctionary Server(Redis) 是一个由 Salvatore Sanfilippo 写的 key-value 存储系统，是跨平台的非关系型数据库。\n<!--more-->\nRedis 是一个开源的使用 ANSI C 语言编写、遵守 BSD 协议、支持网络、可基于内存、分布式、可选持久性的键值对(Key-Value)存储数据库，并提供多种语言的 API。\n\nRedis 通常被称为数据结构服务器，因为值（value）可以是字符串(String)、哈希(Hash)、列表(list)、集合(sets)和有序集合(sorted sets)等类型。\n\nredis中可以存储的数据类型有：  \n## 1.String 字符串\n* 1.1存储命令 \n``set key value``\n* 1.2获取命令\n``get key``\n\n## 2.Hash 散列 \n* 1.存储命令   \n``hmset key field1 field1_value field2 field2_value  ``\n\n* 2.获取命令(根据key和field来获取对应的value)   \n``hmget key filedN``\n\n* 3.获取一个key下面的所有的field和value  \n``hmgetall key``\n\n* 4.获取一个key下面所有的field  \n``hkeys key``\n\n对于redis而言，当需要存储一些结构化的对象时，适合使用hash的类型来进行存储。  \n## 3.List 列表\n\n\nSet 集合 \nSorted Set 有序集合 \n","tags":["redis"],"categories":["cache"]},{"title":"Hadoop基础-HDFS基本原理","url":"/2022/02/18/Hadoop基础-HDFS基本原理/","content":"\n\n概述：Hadoop是Apache软件基金会所开发的并行计算框架与分布式文件系统。其核心主要包括三个模块：Hadoop Common，HDFS与MapReduce。\n<!--more-->\n## 1.Hadoop Common\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n----\n\n\n\n## 2.HDFS\n\n概述：hdfs是hadoop分布式文件系统（Hadoop Distributed File System）的缩写，为分布式计算存储提供底层支持。采用java语言编写，可以部署到多种普通的廉价机器上面，以集群处理数量积累达到大型主机处理性能。Hdfs架构原理采用master/slave架构。一个HDFS集群包含一个单独的Name Node 和多个DataNode。NameNode管理的是元数据，而DataNode存储的是实际的数据。\n\n### 2.1 核心概念\n\n* 1.Block \n\n物理磁盘中有块的概念，磁盘的物理block是磁盘操作的最小单位，一般为512Byte，文件系统的block是抽象在物理block之上的一般是物理block的数倍。通常为数kb，相对于单机系统而言，HDFS的block要大的多，默认的是128M，在hdfs中的文件会被拆分成block大小的chunk，chunk作为独立的单元存储，当存储比block小的文件时，只会占用实际的大小，如1m的文件占用的是1m的存储空间而不是128M。\n\n设置一个大的block主要是为了减少定位磁盘在整个查找数据中的时间占比。\n\n这种block的好处，这种拆分可以存储比整个磁盘容量还要打的文件，因为构成这个文件的block会分布在整个集群上面，理论上要给文件的block拆分到集群中所有的机器的磁盘上面。Block的抽象简化了存储系统，对于Block无需关注他的权限，所有者等内容，这些信息只需要在文件级别上进行控制，同时block也作为容错和高可用的机制中的副本单元，容错的发生是以block为单位进行复制。\n\n* 2.NameNode 和DateNode\n\n相关的已经在概述中进行了说明。\n\n同时nameNode 存放文件系统树和所有文件目录的元数据，元数据持久化主要是两个方式：\n\n* namespace image\n* edit log\n\n主要说明一点在HDFS中NameNode很容易成为集群的单点故障，从而造成整个集群的不可用，为了解决这个问题，主要有以下两个方法：\n\n* a. 备份持久化元数据：将文件系统的元数据同时写到多个文件系统，例如同时将这些元数据保存到本地系统，这些备份的操作都是原子的，同步的。\n* b.Second NameNode ： Second节点定期合并主Namenode的namespace image和edit log， 避免edit log过大，通过创建检查点checkpoint来合并。它会维护一个合并后的namespace image副本， 可用于在Namenode完全崩溃时恢复数据。\n\n对于DataNode来说主要负责存储和提取Block，读写请求可能来自nameNode也可以直接来与客户端，同时DateNode会周期性的向NameNode来提交上传自己的Block相关的信息。\n\n* 3.Block Cache\n\n对于DataNode经常从磁盘中读取的Block，会在内存中对该Block进行缓存，但是一个Block只会缓存一个数据节点上。\n\n* 4.HDFS Federation\n\n这种NameNode和DataNode主从的结构，NameNode的内存大小会制约文件的数量，当开启了HDFS Federation模型时，可以横向的对NameNode进行扩展，使用多个节点来分别管理namespace下面的一部分，例如一个nameNode管理/user目录下面的文件，另外一个nameNode管理/share目录下面的文件，这些所有的nameNode同时来维护一个BlockPool，保存Block中节点的映射信息。\n\n* 5.Hadoop的HA方案\n\n在hdfs集群中，namenode仍然是单点故障的，元数据同时写入到另外的文件系统和Seconde Namenode定期的进行checkpoint有利于保护数据丢失，但是并不能提高可用性。应为当NameNode挂掉之后，常规的做法都是使用元数据备份来重启要给新的那么Node，启动一个新的nameNode耗时会比较久，一般在几十分钟甚至到数个小时。\n\n造成重启耗时的原因大致有：\n1） 元数据镜像文件载入到内存耗时较长。\n2） 需要重放edit log\n3） 需要收到来自DataNode的状态报告并且满足条件后才能离开安全模式提供写服务。\n\n1） 主备需共享edit log存储。\n主NameNode和待命的NameNode共享一份edit log，当主备切换时，Standby通过回放edit log同步数据。\n共享存储通常有2种选择\n\nNFS：传统的网络文件系统\nQJM：quorum journal manager\nQJM是专门为HDFS的HA实现而设计的，用来提供高可用的edit log。QJM运行一组journal node，edit log必须写到大部分的journal nodes。通常使用3个节点，因此允许一个节点失败，类似ZooKeeper。注意QJM没有使用ZK，虽然HDFS HA的确使用了ZK来选举主Namenode。一般推荐使用QJM。\n\n2）DataNode需要同时往主备发送Block Report\n因为Block映射数据存储在内存中（不是在磁盘上），为了在Active NameNode挂掉之后，新的NameNode能够快速启动，不需要等待来自Datanode的Block Report，DataNode需要同时向主备两个NameNode发送Block Report。\n\n3）客户端需要配置failover模式（对用户透明）\nNamenode的切换对客户端来说是无感知的，通过客户端库来实现。客户端在配置文件中使用的HDFS URI是逻辑路径，映射到一对Namenode地址。客户端会不断尝试每一个Namenode地址直到成功。\n\n4）Standby替代Secondary NameNode\n如果没有启用HA，HDFS独立运行一个守护进程作为Secondary Namenode。定期checkpoint，合并镜像文件和edit日志。\n\n如果当主Namenode失败时，备份Namenode正在关机（停止 Standby），运维人员依然可以从头启动备份Namenode，这样比没有HA的时候更省事，算是一种改进，因为重启整个过程已经标准化到Hadoop内部，无需运维进行复杂的切换操作。\n\nNameNode的切换通过代failover controller来实现。failover controller有多种实现，默认实现使用ZooKeeper来保证只有一个Namenode处于active状态。\n\n## 2.2 HDFS数据的读写\n\n### 1.读数据\n\n![image-20210820105303943](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210820105303943.png)\n\n1）客户端传递一个文件Path给FileSystem的open方法\n\n2）DFS采用RPC远程获取文件最开始的几个block的datanode地址。Namenode会根据网络拓扑结构决定返回哪些节点（前提是节点有block副本），如果客户端本身是Datanode并且节点上刚好有block副本，直接从本地读取。\n\n3）客户端使用open方法返回的FSDataInputStream对象读取数据（调用read方法）\n\n4）DFSInputStream（FSDataInputStream实现了改类）连接持有第一个block的、最近的节点，反复调用read方法读取数据\n\n5）第一个block读取完毕之后，寻找下一个block的最佳datanode，读取数据。如果有必要，DFSInputStream会联系Namenode获取下一批Block 的节点信息(存放于内存，不持久化），这些寻址过程对客户端都是不可见的。\n\n6）数据读取完毕，客户端调用close方法关闭流对象\n\n在读数据过程中，如果与Datanode的通信发生错误，DFSInputStream对象会尝试从下一个最佳节点读取数据，并且记住该失败节点， 后续Block的读取不会再连接该节点 \n读取一个Block之后，DFSInputStram会进行检验和验证，如果Block损坏，尝试从其他节点读取数据，并且将损坏的block汇报给Namenode。 \n客户端连接哪个datanode获取数据，是由namenode来指导的，这样可以支持大量并发的客户端请求，namenode尽可能将流量均匀分布到整个集群。 \nBlock的位置信息是存储在namenode的内存中，因此相应位置请求非常高效，不会成为瓶颈。\n\n###　３．写文件\n\n![image-20210820105425780](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210820105425780.png)\n\n1）客户端调用DistributedFileSystem的create方法\n\n2）DistributedFileSystem远程RPC调用Namenode在文件系统的命名空间中创建一个新文件，此时该文件没有关联到任何block。 这个过程中，Namenode会做很多校验工作，例如是否已经存在同名文件，是否有权限，如果验证通过，返回一个FSDataOutputStream对象。 如果验证不通过，抛出异常到客户端。\n\n3）客户端写入数据的时候，DFSOutputStream分解为packets（数据包），并写入到一个数据队列中，该队列由DataStreamer消费。\n\n4）DateStreamer负责请求Namenode分配新的block存放的数据节点。这些节点存放同一个Block的副本，构成一个管道。 DataStreamer将packet写入到管道的第一个节点，第一个节点存放好packet之后，转发给下一个节点，下一个节点存放 之后继续往下传递。\n\n5）DFSOutputStream同时维护一个ack queue队列，等待来自datanode确认消息。当管道上的所有datanode都确认之后，packet从ack队列中移除。\n\n6）数据写入完毕，客户端close输出流。将所有的packet刷新到管道中，然后安心等待来自datanode的确认消息。全部得到确认之后告知Namenode文件是完整的。 Namenode此时已经知道文件的所有Block信息（因为DataStreamer是请求Namenode分配block的），只需等待达到最小副本数要求，然后返回成功信息给客户端。\n\n\n\n\n\n----\n\n\n\n## 3.MapReduce\n\n计算一定时间类的最大温度：\n\nMap程序：\n\n![image-20210719134756064](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210719134756064.png)\n\nReduce程序：\n\n![image-20210719134844464](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210719134844464.png)\n\n![image-20210719134927858](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210719134927858.png)\n\n通过构造JobConf对象来控制实现我们的mapreduce任务，在hadoop集群上执行这个任务时，需要将代码打成要给jar包，在JobConf中传递该类的Class类型，通过调用FiileInpuFormat类的静态函数addInputPath()来定义输入数据的路径，可以是单个文件也可以是目录，将会输入目录下的所有文件。通过调用setOutputPath()来设置reduce函数输出文件的目录，必须是不存在的目录，否则会报错。setMapperClass 和setReducerClass是指定map和reduce类的类型，\n\n![image-20210719141703198](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210719141703198.png)\n\n一个reduce任务的完整流程如图，虚线框表示节点，虚线箭头表示节点内部的数据传输，实线箭头表示节点之间的数据传输。reduce任务的数量不是输入数据的大小决定的，而是特别指定的。如果有多个reduce任务的时候，map任务会将结果进行分区，map的结果键值对要输入到一个reduce节上的会存放在同一个分区中。同时分区由用户定义的分区函数来控制。\n\n![image-20210719142203781](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210719142203781.png)\n\n\n\n","tags":["bigData","hadoop"],"categories":["bigData","hadoop"]},{"title":"MQ消息队列（1）","url":"/2022/02/17/MQ消息队列（1）/","content":"## 1.什么是消息队列 \n消息(message):是指在应用间传递的内容，可以是文本字符串，也可以是一个封装的对象。\n消息队列(message queue):是应用之间的一种通信方式，在通信过程中消息放松之后立即返回，<!--more-->由消息系统来保证消息的可靠传递。对于消息发送者只管将消息发布到消息系统，对于消息消费者只管来消息系统来消费消息，发送者和消费者两者之间不用知道对方的存在。\n\n## 2.当前主要的mq产品\n当前主要的消息队列产品有：老牌的activemq,rabbitmq和当前比较火的kafka,zeromq和阿里baba捐赠给apache基金会的rocketmq\n\n## 3.mq的特点以及使用场景\n从上面的描述中可以看出，消息队列可以应用间一种异步的协作机制。对此，可以使用订单系统来为例，对于一个用户的“下单”请求其中的业务逻辑可能包括开对应的单据，发红包，发送短信通知等等。在业务较简单的时候，可以在服务中将这些逻辑放在一起同步执行；当服务的变的更复杂，同时用户数量增加，需要提升服务的性能，这种情况下就可以使用MQ来增强应用服务的性能。在这种情况下使用MQ，可以将“订单”中的主线程“开对应单据”，“扣减库存”等关键业务完成后，就可以立即返回，同时将其他不重要的业务（发红包,发送短信通知）作为一条消息发送到MQ，由其他单独的线程来拉取MQ中的消息（或者由MQ进行推送）,从而来执行其中的业务。\n以上是用于服务的解耦，提升应用性能。其他的场景还有最终一致性，广播，错峰流控等。\n\n\n\n","tags":["rabbitMq"],"categories":["messageQueue"]},{"title":"MQ消息队列（2）RabbitMQ-AMQP","url":"/2022/02/17/MQ消息队列（2）RabbitMQ-AMQP/","content":"## 1.RabbitMQ概述\n消息队列（Message Queue）起源于一位来自MIT的硬件设计工作者Vivek Randive,其设想一种通用的软件总线，类似电路板上的总线一样，可以供其他软件接入。<!--more-->Vivek在1983年成立了Teknekron公司，高盛等公司作为第一批在金融中使用Teknekorn的软件，同时还诞生了第一代消息队列软件：Teknekron的TIB(THE Information Bus).  \n\n&emsp;&emsp;Teknekron的TIB可以让开发者建立一系列的规则去描述消息内容，只需要将消息按照这个规则发送出去，任何消费者只要订阅了当前的内容，就可以接受到这些发送的消息，从而让生产者和消费者完全解耦，同时可以在消息的传递过程中灵活的结合。这个软件后来后来就受到了电信行业特别是新闻机构的注意。1995年路透社收购了Teknekron公司。  \n\n&emsp;&emsp;由于消息队列在金融交易中取得的不错的反响。1990年开始IBM公司也开始研制自己的消息队列软件（IBM MQ），并且逐步发展成为WebSphereMQ同时占领着广阔的商业消息队列平台市场。于此同时，微软也开始了消息队列的研制MSMS（Microsoft Message Queue）应运而生，随着众多厂商参与MQ的研制，同时带来了一个问题，各个厂商的MQ无法进行互通。为了解决这个问题，JMS（java message service）在2001年诞生了，其试图通过提供公共java API的方式来隐藏各个供应商提供的MQ实际的接口，从而解决各个MQ之间互通的问题，但是由于使用单独的标准化接口来胶合不同的接口应用程序反而变的更加脆弱，最终失败了。    \n\n&emsp;&emsp;但在2004年JPMorgan Chase和IMatix公司一起合作开发了Advanced Message Queueing Protocl（AMQP,高级消息队列协议）,从一开始就设计成一个开放标准协议，任何公司都可以使用这个协议来设计自己的MQ产品，这样任何使用这种协议的MQ就可以进行互通使用。RabbitMQ就是基于AMQP协议进行的一个MQ实现。\n\n\n## 2.AMQP（Advanced Message Queuing Protocol）高级消息队列协议\n### 2.1 AMQP协议简介   \n&emsp;&emsp;AMQP(advanced message queue protocol)高级消息队列协议,是一个公开的应用层协议标准，基于此协议可以实现客户端和中间件传递消息并不受客户端和消息中间件的不同产品，不同语言的制约。以下简单介绍一些其中的重要组成：    \n\n![20220118155221](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/20220118155221.png)  \n&emsp;&emsp;由于amqp是一种网络协议，所以这个过程中的生产者，消费者，消费代理可以存在于不同的设备之上。AMQP 0-9-1的工作流程如上，消息和发布者发送给交换机，交换机通常被比喻成邮局，然后交换机将收到的消息根据路由规则发给绑定的队列，最后AMQP代理会将消息投递给订阅了当前队列的消费或者消费者按照自己的需求来使用。  \n\n### 2.2 Exchange(交换机)\n* 2.2.1 默认交换机（default exchange）  \n  &emsp;&emsp;默认交换机是一个由消息队列预先声明好的没有名字的直连交换机,对于默认交换机而言，新建的每一个队列都会自动的绑定到这个默认交换机上面，绑定的路由键（routing key）名称和队列名称相同。  \n  &emsp;&emsp;例如：当声明一个“queueA”的队列，AMQP会自动地将其绑定到默认交换机上面，同时绑定的路由名称键名也是“queueA”，因此当携带有一样的routing key的消息来时就会被路由到queueA的队列中。  \n\n* 2.2.2 直连交换机播路由（unicast routing）。  \n  &emsp;&emsp;1）将一个队列A绑定到某个交换机上面，赋予该绑定一个绑定键（binding key），为R  \n  &emsp;&emsp;2）当一个携带路由键R的消息来到交换机，就会把该消息发送到队列A中。  \n  ![20220118150124](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/20220118150124.png)  \n  &emsp;&emsp;直连交换机的队列通常是循环发送任务给多个消费者，轮询的方式。\n  \n* 2.2.3 扇形交换机  \n![20220118151156](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/20220118151156.png)  \n  &emsp;&emsp;扇形交换机（funout exchange）将消息路由绑定给到他身上所有的队列中，而不会去管路由键（routing key），如果N个队列绑定到一个扇形路由器上面，当有消息发送到这个路由器上时，交换机会将消息发送到所有的N个队列中。扇形交换机用来处理消息的广播路由（broadcast routing）。  \n  &emsp;&emsp;由于扇形路由器的这种特性，和一些的使用场景非常像：  \n  &emsp;&emsp;1）大规模用户在线游戏使用它来处理排行榜更新等全局事件；   \n  &emsp;&emsp;2）体育新闻网站用来近乎实时地将比分更新到移动客户端；   \n  &emsp;&emsp;3）分发系统用它来广播各种状态和配置更新；  \n\n* 2.2.4 主题交换机  \n  &emsp;&emsp;主题交换机（topic exchage）与之前的之前交换机较复杂，但是可以处理的场景也更加灵活。本质还是一个直连交换机，同样需要将传来的消息中的routing key和队列绑定的binding key进行对应，不过可以对其中的key进行如正则匹配的处理。将原有的一个key按照'.'进行切分成多个单词，每个单词可以用“*”，“#”这两个关键字进行匹配。   \n  &emsp;&emsp;其中的*表示一个单词，#表示匹配零个或者多个单词。  \n  如：主题的定义是根据动物不同特点来分发 速度.颜色.名称.  \n  &emsp;&emsp;队列A根据binding key：“*.red.*”绑定到交换机上  \n  &emsp;&emsp;队列B根据binding key：“low.#”和“*.yellow.*”绑定到主题交换机上面   \n  使用上面的方法，可以将所有的红色动物的消息发送到队列A中，将所有的跑到慢的动物和黄色的动物消息发送到队列B中来消费。  \n![20220118152332](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/20220118152332.png)  \n\n### 2.3 Queue 队列  \n&emsp;&emsp;AMQP中的队列跟其他消息队列中的队列是相似的，其中存储着需要消费的消息。  \n* 2.3.1 队列属性   \n  队列属性和交换机共享一些属性，但也有一些是其特有的  \n  1）Name  \n  2）Durable 队列持久化，消息队列重启后队列是否依旧存在，注意只可以保证其队列的持久化，不能保证队列中原还没有消费的消息也持久化    \n  3）Exclusive 只被一个连接，当连接关闭时队列即被销毁  \n  4）Auto-delete 当最后一个消费者退订后即被销毁  \n  5）Arauments 一些消息代理用它来完成类似与TTL的某些额外功能  \n\n* 2.3.2 队列的创建  \n  &emsp;&emsp;队列在声明后才可以被使用，如果一个队列开始不存在在声明时会对它进行创建，如果队列已经存在了而且和声明的属性相同，那么这个声明就不会对原有的队列有任何影响，如果和原有的队列属性有差异，那么就会抛出406的通道异常。  \n\n* 2.3.3 队列持久化  \n  &emsp;&emsp;持久化话队列会将队列的信息存储在硬盘上，当消息代理broker重启的时候，会依旧存在，没有被持久化队列被称之为暂存队列Transient queues,并不是所有的场景和案例都需要持久化队列。  \n  &emsp;&emsp;注意：持久化队列并不会将路由到当前都列的消息也持久化。\n\n### 2.4 消费者Consumer   \n&emsp;&emsp;消息如果只是被存储在队列中不被消费是没有意义的，在amqp 0-9-1模型中，有两种途径可以让消费者来消费队列中的消息：  \n1）将消息投递给消费者应用（push API）  \n2) 消费者应用主动地获取消息（pull API）  \n&emsp;&emsp;使用push的消费者应用需要明确表达它对某个特定的队列感兴趣，可以规定为该队列注册了一个消费者，或者叫该消费者订阅了一个队列，注意一个队列可以注册多个消费者，也可以注册一个独享的消费者，当独享消费者存在时，其他的消费者即被排除在外。  \n&emsp;&emsp;每个消费者都有一个消费者标签的标识符，可以用来退订消息，消费者标签实际上就是一个字符串。  \n\n### 2.5 消息确认  \n* 2.5.1 消息确认  \n  &emsp;&emsp;由于AMQP是一个网络协议，在处理消息的时候会碰见各种因为网络问题消费丢掉的问题，所以需要我对消息进行确认。在AMQP中有两种方法来对消息进行确认：  \n  &emsp;&emsp;1）自动确认模式：当消息代理broker将消息发送后立即删除消息,basic.deliver或者basic.get-ok。  \n  &emsp;&emsp;2）显示确认模式：当broker发送一个消息之后，需要等待一个确认回执acknowledgement后再删除消息,basic.ack。  \n  &emsp;&emsp;如果是第二种方法，当一个消费者当在未发送回执的情况下挂掉了，那么broker代理会将消息重新投递给另外的一个消费者，如果没有其他的消费者，消息代理会等待下一个订阅当前队列的消费者，然后再次尝试投递。  \n* 2.5.2 拒绝消息  \n  &emsp;&emsp;当一个消费者拒绝消息之后，消费者可以告诉消息代理如何处理这条消息-销毁或者重新返回队列中。  \n  &emsp;&emsp;注意：当只有一个消费者订阅当前队列，确实不要因为消费者拒绝了消息放回在队列中又重新返回给这个消费者，造成不断循环拒绝放回。   \n  &emsp;&emsp;在AMQP中,basic.reject方法用来拒绝消息的操作，但是有个限制不能用来拒绝多个带有确认回执的消息，如果使用的是RabbitMQ可以使用nacks来解决这个问题。  \n\n* 2.5.3 预读消息  \n  &emsp;&emsp;在多个消费者共享一个队列的案例中，明确指定在收到下一个确认回执前每个消费者一次可以接受多少条消息是非常有用的，可以在视图批量发布消息的时候起到简单的负载均衡和提供吞吐量的作用。RabbitMQ只支持通道级的预取计数，而不是连接级的预取。   \n\n* 2.5.4 消息属性  \n  &emsp;&emsp;AMQP模型中的消息是带有属性的，常见的有以下这些：  \n  |类型|字段|名称|\n  |----|----|----|\n  |Content|type|内容类型|\n  |Content|encoding|内容编码| \n  |Routing|key|路由键|\n  |Delivery|mode|persistent or not|\n  |Message|priority|消息优先权|\n  |Message|publishing|timestamp|\n  |Expiration|period|消息有效期|\n  |Publisher|application id|发布应用的id|\n\n* 2.5.5 消息主体  \n  &emsp;&emsp;AMQP的消息除了属性之外还有一个有效载荷Payload消息实际携带的数据，被AMQP代理当成一个不透明的字节数组来对待。消息代理不会检查或者修改有效载荷，消息可以只包含属性而不携带任何有效载荷，通常会使用类似JSON这种序列化的格式数据，为了节省传输的数据大小，协议为缓冲器会和MessagePack将结构化的数据序列化，以便以消息的有效载荷方式来传递。通常会用content-type和content-encoding这两个字段来与消息沟通有效载荷的辨识工作。  \n\n* 2.5.6 消息持久化  \n  &emsp;&emsp;消息也能够被持久化，AMQP会将消息保存到磁盘上，如果代理重启，系统会确认保存的消息未丢失。  \n  &emsp;&emsp;需要注意的是：简单的对队列持久化并不会让其中的消息也持久化，凶啊系的持久化需要设置消息自己的模式persistence mode,将消息持久化之后自然会影响性能，所以需要在根据自己的情景来设置是否需要对消息进行持久化，健壮性的增加自然需要牺牲一下性能。    \n\n* 2.5.7 连接  \n  &emsp;&emsp;AMQP的连接通常是长连接，AMQP是使用TCP提供可靠的应用层协议。AMQP使用认证机制并且提供TLS保护，当一个应用不需要连接到AMQP代理的时候，只需要释放到AMQP的连接，而不是直接将TCP连接关闭。  \n\n* 2.5.8 通道  \n  &emsp;&emsp;在有些系统中需要与AMQP建立多个连接，但是同时开启多个TCP连接会消耗很多资源，因此AMQP提供了通道Channels来处理多连接，可以将通道看成一个TCP连接的多个轻量化连接。  \n  &emsp;&emsp;在涉及多线程或者多进程的系统中，为每个线程开启一个通道是常见的，并且这些通道之间不会被共享资源。   \n* 2.5.9 虚拟主机vhost    \n\n  \n","tags":["rabbitMq"],"categories":["messageQueue"]},{"title":"MQ消息队列（3）RabbitMQ","url":"/2022/02/17/MQ消息队列（3）RabbitMQ/","content":"## 1.RabbitMQ安装\n* 1.安装对应的erlang版本，需要去RabbitMQ官网上查看对应的版本安装，一般是去下编译好的，然后对对应的rpm包安装 \n<!--more-->\n  ``rpm -Uvh erlang-solutions-2.0-1.noarch.rpm``\n\n* 2.安装对应的RabbitMQ版本``rpm -Uvh rabbitmq-server-3.8.22-1.el7.noarch.rpm``  \n\n* 3.配置rabbitmq的基本信息\n  ```sh\n    rabbitmqctl list_users # 查看rabbitmq用户列表,默认用户名密码都为guest\n\n    rabbitmqctl add_user admin1 123456  # 新增用户 \n\n    rabbitmqctl add_user <username> <password>\n    rabbitmqctl delete_user <username>\n    rabbitmqctl change_password <username> <newpassword>\n    rabbitmqctl clear_password <username>\n    rabbitmqctl authenticate_user <username> <password>\n    rabbitmqctl set_user_tags <username> <tag> ...\n    rabbitmqctl list_users\n    # 给创建的用户配置权限 \n    rabbitmqctl add_user admin StrongPassword\n    rabbitmqctl set_user_tags admin administrator\n    rabbitmqctl set_permissions -p / admin “.*” “.*” “.*”\n    # 开启web监控平台 \n    rabbitmq-plugins enable rabbitmq_management\n    #使用之前的用户名登陆\n    http://ip:15672\n  ```\n* 4.监控页面\n  ![20220119152307](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/20220119152307.png)\n\n\n## 2.Rabbit的helloworld   \n","tags":["rabbitMq"],"categories":["messageQueue"]},{"title":"MQ消息队列（4）RabbitMQ源码","url":"/2022/02/17/MQ消息队列（4）RabbitMQ源码/","content":"## 1.概述   \n在RabbitMQ客户端中使用的顶级包com.rabbitmq.client;其中主要的接口和类有：   \n<!--more-->\n（1）Channel:AMQP0-9-1的通道，提供了大部分的操作和方法   \n（2）Connection:AMQP 0-9-1的连接  \n（3）ConnectionFactory：用于构建Connection的实例   \n（4）Consumer：消息的消费者  \n（5）DefaultConsumer：消费者常用的基类  \n（6）ba\n\n## 2.源码解析   \n* 1.Consumer 接口 \n```java\n\npackage com.rabbitmq.client;\nimport java.io.IOException;\npublic interface Consumer {\n\n    void handleConsumeOk(String consumerTag);\n\n\n    void handleCancelOk(String consumerTag);\n\n\n    void handleCancel(String consumerTag) throws IOException;\n\n    void handleShutdownSignal(String consumerTag, ShutdownSignalException sig);\n\n    void handleRecoverOk(String consumerTag);\n\n\n    void handleDelivery(String consumerTag,\n                        Envelope envelope,\n                        AMQP.BasicProperties properties,\n                        byte[] body)\n        throws IOException;\n}\n\n```\n","tags":["rabbitMq"],"categories":["messageQueue"]},{"title":"java代理Proxy","url":"/2022/02/14/java代理Proxy/","content":"\n代理为什么使用代理，代理的主要作用就可以在不改动原有代码的基础上给原有对象添加新的功能，实现的方法是被代理类和代理类继承同一个类或者实现了同一个接口，java中代理分为静态代理和动态代理：\n<!--more-->\n## 1.静态代理\n\n静态代理中的静态主要表现在代理类是在提前写好的，也可以说所使用的代理在程序运行开始之前就知道，代理对象是通过new出来的。我们的被代理对象是一个自行车，后面对车进行了升级，可以在上面使用电动的骑行。\n\nMachine.java\n\n````java\npublic interface Machine {\n    void drive();\n}\n````\n\nBicycle.java\n\n```java\npublic class Bicycle implements Machine{\n    private String name;\n    public Bicycle() {\n        this.name = \"人力\";\n    }\n    public String getName() {\n        return name;\n    }\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    @Override\n    public void drive() {\n        System.out.println(\"我正在使用\"+this.name+\"骑行\");\n    }\n}\n```\n\nEleBicycle.java\n\n```java\npublic class EleBicycle implements Machine{\n    private Bicycle bicycle;\n    public EleBicycle(Bicycle bicycle) {\n        this.bicycle = bicycle;\n    }\n\n    @Override\n    public void drive() {\n        System.out.println(\"现在升级了电力，可以用电力骑行\");\n        this.bicycle.setName(\"电力\");\n        this.bicycle.drive();\n    }\n}\n```\n\nPeople.java\n\n```java\npublic class People {\n\n    public static void main(String[] args) {\n        Bicycle bicycle = new Bicycle();\n        bicycle.drive();\n\n        // 使用代理\n        Machine bicycleProxy = new EleBicycle(bicycle);\n        bicycleProxy.drive();\n    }\n}\n```\n\n![image-20210716111029245](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210716111029245.png)\n\n他们直接的关系可以看成如图所示，自行车（Bicycle）和加了电的自行车（EleBicycle）分别是被代理的对象和代理，他们都实现了Machine这个接口，然后People这个用户来消费这个产品。\n\n通过上面的例子可以明显看出来使用代理的<u>**好处**</u>就是当我们需要对我们的类进行功能升级时不用直接在写好的类上面重新加功能代码，因为可能在有的地方是需要升级过的类，有的地方是需要未升级的类。使用代理的方式也使代码得到了复用。但使用静态代理也<u>**存在一些缺点**</u>：使用静态代理需要我们在代码运行之前就需要知道用户需要新加的功能是在那个代理类里面，但在实际开发中会存在在开发的时候，开发人员不知道用户需要的操作要使用哪个代理类，只有当用户点击页面时才知道用户所要使用的代理类，这种代理也就是我们所要学到的动态代理。\n\n## 2.动态代理\n\n动态代理的使用情景在于开发人员无法在程序运行开始前知道用户所要使用的代理是那个，这种在程序运行中创建对象的方法自然就想到了反射。在java的动态代理中最重要的是一个接口InvocationHandler，我们的代理类要实现这个接口同时重写其中的invoke（）方法，和一个Proxy类中的newProxyInstance()静态方法用来动态的创建代理类。动态代理中的代理类不要我们手动写，只需要些一个实现了InvocationHadler接口的类，通过Proxy会自动根据我们的handler来创建代理类。\n\nHello.java\n\n```java\npublic interface Hello {\n    public String hello();\n\n    public String bye();\n}\n```\n\nHelloWorld.java\n\n````java\npublic class HelloWorld implements Hello{\n    @Override\n    public String bye() {\n        System.out.println(\"这是被代理对象中的bye\");\n        return \"bye\";\n    }\n\n    @Override\n    public String hello() {\n        System.out.println(\"这是被代理对象中已有的方法\");\n        return \"hello\";\n    }\n}\n````\n\nWorkHandler.java\n\n````java\nimport java.lang.reflect.InvocationHandler;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Proxy;\npublic class WorkHandle implements InvocationHandler {\n    // 被代理的对象\n    private Object object;\n\n    // 代理类的构造方法用于给被代理类实例赋值\n    public WorkHandle(Object object) {\n        this.object = object;\n    }\n\n    @Override\n    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n        System.out.println(\"代理类中的新功能\");\n        if(method.getName()==\"bye\"){\n            System.out.println(\"代理类中的bye\");\n        }\n        Object invoke = method.invoke(object,args);\n\n        return invoke;\n    }\n\n    public static void main(String[] args) {\n        Hello people = new HelloWorld();\n        InvocationHandler invocationHandler = new WorkHandle(people);\n        Hello proxy = (Hello) Proxy.newProxyInstance(invocationHandler.getClass().getClassLoader(),\n                people.getClass().getInterfaces(),\n                invocationHandler);\n        String hello = proxy.hello();\n        System.out.println(hello);\n        String bye = proxy.bye();\n        System.out.println(bye);\n    }\n}\n````\n\n上面就是一个动态代理的实现过程，我们还可以写不同的handler，运行中可以根据不同的handler来实现不同的代理。下面将Proxy和InvocationHandler展开细看。\n\n### 1.Proxy类\n\n其中的newProxyInstance（）\n\n````java\n@CallerSensitive\n\t// 类加载器，实现的接口，handler\n    public static Object newProxyInstance(ClassLoader loader,\n                                          Class<?>[] interfaces,\n                                          InvocationHandler h)\n        throws IllegalArgumentException\n    {\n        Objects.requireNonNull(h);\n        final Class<?>[] intfs = interfaces.clone();\n        final SecurityManager sm = System.getSecurityManager();\n        if (sm != null) {\n            checkProxyAccess(Reflection.getCallerClass(), loader, intfs);\n        }\n\n        /*\n         * Look up or generate the designated proxy class.\n         */\n        Class<?> cl = getProxyClass0(loader, intfs);\n\n        /*\n         * Invoke its constructor with the designated invocation handler.\n         */\n        try {\n            if (sm != null) {\n                checkNewProxyPermission(Reflection.getCallerClass(), cl);\n            }\n\n            final Constructor<?> cons = cl.getConstructor(constructorParams);\n            final InvocationHandler ih = h;\n            \n            if (!Modifier.isPublic(cl.getModifiers())) {\n                AccessController.doPrivileged(new PrivilegedAction<Void>() {\n                    public Void run() {\n                        cons.setAccessible(true);\n                        return null;\n                    }\n                });\n            }\n            // 通过获取的构造方法来生成类的对象，即代理对象\n            return cons.newInstance(new Object[]{h});\n        } catch (IllegalAccessException|InstantiationException e) {\n            throw new InternalError(e.toString(), e);\n        } catch (InvocationTargetException e) {\n            Throwable t = e.getCause();\n            if (t instanceof RuntimeException) {\n                throw (RuntimeException) t;\n            } else {\n                throw new InternalError(t.toString(), t);\n            }\n        } catch (NoSuchMethodException e) {\n            throw new InternalError(e.toString(), e);\n        }\n    }\n\n````\n\n### 2.InvocationHandler\n\n````java\n//里面就一个待实现的invoke（）方法\npublic interface InvocationHandler{\n    public Object invoke(Object proxy,Method method,Object[] args) throws Throwable;\n}\n````\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n​             \n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["java基础"],"categories":["java"]},{"title":"java反射","url":"/2022/02/14/java反射/","content":"## 1.java反射\n\n在java中每个类都有其对应的Class，我们可以在程序中通过其Class对象来获取该类的基本信息，生成该类的对象，运行该类的方法。\n\n<!--more-->\n为了理解反射首先来了解一下什么事Class类，其实就是类的类，在java中我们会将所有编程的对象抽象成一个类也就是一个class，这个class就含有这类事物所具有的一些基本信息（属性）和行为（方法），而对这些我们定义的类class而言，java将所有这些我们定义的类又进行了一层抽象Class也就是类的类。显而易见，在Class会包含类这个定义的一写基本信息，类名，类访问类型，属性名，属性类型，方法名，方法返回值类型，方法参数类型，构造函数，构造函数参数类型，构造函数参数类型等等。下面就是java中所定义的Class类。\n\n同时为什么要用反射，这是我之前一直没想过的问题，后面想想觉得主要是两个原因。\n\n* 1.首先new出来的对象，我们是无法使用其中的私有属性和方法，但是对于反射而言，我们任然可以获取到其中的私有属性的资源。\n* 2.在编程开发时我们不知道需要使用的类名称，所以也就无法使用new 来创建（非要这样的话，也可以提前写好不同条件的判断来new不同的对象，显然这样的程序冗余很多看着头大，设计不合理的），只有当用户使用我们的程序时，这个时候根据用户的操作不同，会使用不同的类，这个时候已经无法在程序中使用new来创建对象，只能根据对应的类名使用反射的方法来创建该类的对象实例来满足用户的需求。\n\n可见使用反射的目的就是使用反射获取类的对象并使用该对象的一些方法。我们可以简单将其分为一下三个步骤：（获取Class对象，生成Class 对象实例，获取对象的方法，属性并使用）\n\n* 1.获取Class对象有三种方法：\n\n  * a.使用Class的forName()方法\n\n    ```java\n    Class catClass = Class.forName(\"com.bevis.Cat\");\n    ```\n\n  * b.使用getClass()方法，这个是Object的方法\n\n    ```java\n    Class catClass = Cat.getClass();\n    ```\n\n  * c.使用该对象的.class(）方法来获取\n\n    ```java\n    Cat cat = new Cat();\n    Class catClass = cat();\n    ```\n\n* 2.获取对象实例两种方法\n\n  * a.通过Class对象来\n\n    ```java\n    Object ojc = catClass.newInstance();\n    ```\n\n  * b.使用构造函数来创建\n\n    ```java\n    Constructor con = catClass.getConstructor();\n    Object o = con.newInstance();\n    ```\n\n    使用Class对象来创建类对象默认会使用无参的构造函数创建类对象。\n\n* 3.使用类对象的方法\n\n  ````java\n  Method say = catClass.getMethod(\"say\",String.class);\n  String bye = say.invoke(obc,\"bye--\");\n  // 加入访问的方法是private修饰时，需要下面的方法来获取方法和使用该方法,当获取的是私有方法是需要使用getDeclaredMethod()方法并将setAccessible（）为true\nMethod testPrivate = catClass.getDeclaredMethod(\"testPrivate\",String.class);\n  testPrivate.setAccessible(true);\n  String re = testPrivate.invoke(ojc,\"---testPrivate---\");\n  ````\n  \n  其中在Class对象中除了以上用到的属性和方法还有以下：\n  \n  还有通过反射获取到该Class对象中的注解的getAnnnotations()，也可以通过Class中的属性Field,Method等来使用getAnnotation（）获取到对应元素上的注解的动态代理类对象。\n\n","tags":["java基础"],"categories":["java"]},{"title":"java注解的实现原理（1）","url":"/2022/02/14/java注解的实现原理（1）/","content":"\n**注解的本质就是一个继承了Annotation接口的接口**\n\n写在前面，在前面总结了java反射和动态代理的一些知识，同时之前没有仔细研究注解这块，只知道注解的实现原理是基于动态代理的，主要作用有一下：\n<!--more-->\n* 1.编译检查：例如使用@SupperssWarnings,@Override都具有编译检查的作用。\n* 2.可以帮助生成文档，例如@Return @Param等注解。\n* 3.在框架中替换之前的xml文件使用注解开发web，例如spring中的各种注解。\n\n之前的理解一直差不多这种，最近刚好不忙想相似的了解一下java注解的实现原理，在开始看自定义注解中，看到很多博客里面写的都是如下这样：\n\n自定义注解Info.java\n\n````java\n@Target(ElementType.FIELD)\n@Retention(RetentionPolicy.RUNTIME)\npublic @interface Info{\n\tString value();    \n}\n````\n\n使用自定义的注解：People.java\n\n````java\npublic class People{\n   @Info(\"张三\") \n   private String name;\n    \n   public String getName(){\n       return this.name;\n   } \n    \n   public void setName(String name){\n       this.name = name;\n   }\n}\n````\n\n然后一般到这里就完了，但是当我照着这样写了一遍再用测试类来运行生成People类对象并调用getName（）方法时，我们并得不到我们设置的张三，同时我也看不到这个动态代理在哪里。为此有翻阅了好久的资料发现注解不是这样用的。我们自定义的注解其实还需要一个中间配置类来配置的我的注解解析。\n\n如下我们自定义了2个注解@LoadProperty 用来配置加载我们的配置文件，@ConfigField用来配置下面的字段赋值。中间用来配合注解解析的类AnnoResolve.java，使用这两个注解的User.java和测试主函数存在的类TestUser.java。具体代码实现如下所示：\n\n@LoadProperty 用来配置加载我们的配置文件\n\n````java\nimport java.lang.annotation.*;\n\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\npublic @interface LoadProperty {\n    String value(); // 配置文件的路径\n}\n````\n\n@ConfigField用来配置下面的字段赋值\n\n````java\nimport java.lang.annotation.*;\n\n@Target(ElementType.FIELD)\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\npublic @interface ConfigField{\n    String value();\n}\n````\n\n使用以上两个注解的User.java\n\n```java\npackage com.ths.annotaion;\n\n@LoadProperty(\"D:\\\\JAVA_HOME\\\\bevisStudy\\\\src\\\\com\\\\ths\\\\annotaion\\\\config.properties\")\npublic class User {\n    //在类中使用注解\n\n    @ConfigField(\"user.id\")\n    private String id;\n\n    @ConfigField(\"name\")\n    private String name;\n\n    public String getId() {\n        return id;\n    }\n\n    public void setId(String id) {\n        this.id = id;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n}\n```\n\n中间配置类AnnoResolve.java\n\n```java\npublic class AnnoResolve {\n\n    public static <T> void loadProperty(T t){\n        // 通过传入的user对象来获取User的Class对象cls\n        Class<? extends Object> cls =t.getClass();\n        // 通过isAnnotationPresent()方法判断LoadProperty注解是否存在于此元素上面\n        boolean hasLoadPropertyAnno = cls.isAnnotationPresent(LoadProperty.class);\n        if(hasLoadPropertyAnno){\n            //为属性赋值\n            try {\n                configField(cls,t);\n            } catch (IOException e) {\n                e.printStackTrace();\n            } catch (IllegalAccessException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n    private static <T> void configField(Class<? extends Object> cls,T t) throws IOException, IllegalAccessException {\n        // 获取到cls资源上的注解代理类，使用其中的value（）方法得到其中的配置文件路径\n        String filePath = cls.getAnnotation(LoadProperty.class).value();\n        Properties properties = new Properties();\n//        InputStream is = AnnoResolve.class.getClassLoader().getResourceAsStream(filePath);\n        InputStream is = new FileInputStream(filePath);\n        System.out.println(is);\n        properties.load(is);\n        // 通过反射获取到user上面的字段\n        Field[] fields = cls.getDeclaredFields();\n        for (Field field : fields) {\n            // 遍历找到字段上含有注解的字段\n            boolean hasConfigField = field.isAnnotationPresent(ConfigField.class);\n            String fieldValue = null;\n            // 如属性上有注解，使用注解的值作为key去配置文件中查找\n            if(hasConfigField){\n                // 获取注解的值\n                Object annoValue = field.getAnnotation(ConfigField.class).value();\n                fieldValue = properties.getProperty(annoValue.toString());\n            // 如属性上没有值\n            }else{\n                fieldValue = properties.getProperty(field.getName());\n            }\n            // 如果是私有成员变量需要设置为true\n            field.setAccessible(true);\n            field.set(t,fieldValue);\n        }\n        is.close();\n    }\n\n}\n```\n\n测试注解在User中使用的TestUser.java\n\n```java\npublic class TestUser {\n\n    public static void main(String[] args) {\n        User user = new User();\n        AnnoResolve.loadProperty(user);\n        System.out.println(user.getId());\n        System.out.println(user.getName());\n    }\n}\n```\n\n其中还要写一个config.properties配置文件\n\n![image-20210720151551278](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210720151551278.png)\n\n其所有在的位置就是在User中需要在注解上传入的值。通过运行TestUser就可以得到如下结果：\n\n![image-20210720151650255](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210720151650255.png)\n\n到此说明通过注解的方法，我们确实给我们创建的User对象赋了我们在配置文件中设置的字符，自定义注解使用成功，下面我们就详细分析一下定义注解的一个过程：\n\n* 1.注解的定义方式使用@interface关键字，注解里面的都是定义的方法，后面会在通过动态代理的方法实现该注解的代理类，然后调用代理类中的value（）方法来获取对应的值，一个注解中定义有多个函数时，在使用注解是需要显示的赋值如下所示。\n\n  ```java\n  public Test{\n      // 加入定义的InfoTest注解里面有两个函数value1和value2，使用时需要显式的进行赋值\n      @InfoTest(value1=\"name\",value2=\"test\")\n      private String name;\n  }\n  ```\n\n* 2.在自定义注解时，注解上面的是java的一些元注解，作用如下：\n\n  ````java\n  @Target()  //表示该注解使用的位置，一般有类，方法，字段，构造函数\n  ElementType.FIELD 使用在属性字段上\n  ElementType.Type 使用在类上\n  ElementType.METHOD 使用在方法上    \n  ElementType.PARAMETER 使用咋方法的参数上\n  ElementType.CONSTRUCTOR 使用在构造函数上\n  ElementType.ANNOTATION_TYPE 使用在注解上\n  ElementType.PACKAGE 使用在包上\n  ElementType.LOCAL_VARIABLE 使用在本地局部变量上\n      \n  @Retention()  // 定义该注解的生命周期\n  SOURCE：注解只保留在源文件，当Java文件编译成class文件的时候，注解被遗弃；\n  CLASS：注解被保留到class文件，但jvm加载class文件时候被遗弃，这是默认的生命周期；\n  RUNTIME：注解不仅被保存到class文件中，jvm加载class文件之后，仍然存在；\n  \n  @Doucumented // 表示该注解会写入到文档中\n  \n  @Inherited  // 表示定义的这个注解当标注在一个类上时，当其有其他子类来继承时，子类也会自动继承标注在父类上的注解\n  ````\n\n* 3.注解的配置类这一部分其实是真正注解给注解的类赋值的实现过程。\n  * a.首先通过传入的user获取到User的Class 对象cls，判断类上面是否有LoadProperty注解，同时可以通过cls.getAnnotation(传入注解的Class对象)可以得到java动态代理生成该注解的代理类，然后使用对应的value()方法获取到前面，我们使用注解传入的配置文件路径。（这一部分的实现源码，在后面的博客中继续分析）\n  * b.有的话，通过cls对象反射获取到user中所有的成员字段，遍历所有的字段，使用同样的isAnnotationPresent（）方法判断在该元素上时候有我们需要的注解，用需要的方法来取我们需要用到的值。\n  * c.通过cls对象使用发射，如果是字段使用set方法来需要的字段赋值刚才通过注解获取到的值。（配置到方法上的以后再看）\n\n以上只是自定义注解的使用整体理解，对于其中的更近一层的细究在后面继续进行。如在getAnnotation（）在java源码中是如何生成动态代理中。在程序使用debug你会发现通过getAnnotation（）返回的对象属性是$Prxoy1，不想继续往下看，这样记住返回出来的值就是一个该注解的一个动态代理的代理类。\n","tags":["java基础"],"categories":["java"]},{"title":"java中枚举的使用","url":"/2022/02/14/java中枚举的使用/","content":"\njava中的枚举是在java1.5加入的新功能。\n\n枚举是一种特殊的类对象，定义{修饰符} enum 对象标识 [父接口] 枚举体；\n<!--more-->\n## 1.特点：\n\n* 1.枚举不可以定义为final或者是abstract类型，否则会出现编译错误；\n* 2.枚举类型实现了Compareable和Serializable接口，可以进行比较和序列化操作。\n* 3.枚举类型只能通过内部的枚举常量进行初始化；\n* 4.枚举类型的clone（）方法定义为final，不可以复制，否者会抛异常\n* 5.枚举类型无法通过反射来进行赋值初始化；\n* 6.枚举需要自己来定义序列化和反序列化，默认的序列化会抛出异常；\n* 7.枚举类型的equal方法为final，不可以被自定义覆盖，同关联hashCode方法\n* 8.枚举类型的finalize定义为fina，意味着枚举类型永远不会被\n  Gc回收。\n\n## 2.枚举类型的初始化\n\n```java\npublic enum EnvEnum{\n    PROD(\"project-prod\",\"8080\"),\n    RELEASE(\"project-release\",\"8081\"),\n    DEV(\"project-dev\",\"8082\");\n    \n    private String appId;\n    private String appPort;\n    \n    EnvEnum(String appId,String appPort){\n        this.appId = appId;\n        this.appPort = appPort;\n    }\n    \n    public String getAppId(){\n        return this.appId;\n    }\n    \n    public void setAppId(String appId){\n\t\tthis.appId = appId;        \n    }\n    \n    public String getAppPort(){\n        return this.appPort;\n    }\n    \n    public void setAppPort(String appPort){\n        this.appPort = appPort;\n    }\n}\n```\n\n\n\n其主要的使用场景有如下7个方面：\n\n##　１．作为常量使用\n\n\n\n","tags":["java基础"],"categories":["java"]},{"title":"java注解的实现原理（2）","url":"/2022/02/14/java注解的实现原理（2）/","content":"\n在上文中主要概括介绍了自定义注解的实现原理，本文将主要介绍通过getAnnotation（）使用动态代理生成注解对应代理类的源码实现过程。\n<!--more-->\ncls.getAnnotation(注解.class);\n\n![image-20210720201454411](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210720201454411.png)\n\n![image-20210720201954690](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210720201954690.png)\n\n![image-20210720202029815](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210721145522030.png)\n\n![image-20210721145603193](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210721145603193.png)\n\n````java\nprivate static Annotation parseAnnotation2(ByteBuffer var0, ConstantPool var1, Class<?> var2, boolean var3, Class<? extends Annotation>[] var4) {\n        int var5 = var0.getShort() & '\\uffff';\n        Class var6 = null;\n        String var7 = \"[unknown]\";\n\n        try {\n            try {\n                var7 = var1.getUTF8At(var5);\n                var6 = parseSig(var7, var2);\n            } catch (IllegalArgumentException var18) {\n                var6 = var1.getClassAt(var5);\n            }\n        } catch (NoClassDefFoundError var19) {\n            if (var3) {\n                throw new TypeNotPresentException(var7, var19);\n            }\n\n            skipAnnotation(var0, false);\n            return null;\n        } catch (TypeNotPresentException var20) {\n            if (var3) {\n                throw var20;\n            }\n\n            skipAnnotation(var0, false);\n            return null;\n        }\n\n        if (var4 != null && !contains(var4, var6)) {\n            skipAnnotation(var0, false);\n            return null;\n        } else {\n            AnnotationType var8 = null;\n\n            try {\n                var8 = AnnotationType.getInstance(var6);\n            } catch (IllegalArgumentException var17) {\n                skipAnnotation(var0, false);\n                return null;\n            }\n\n            Map var9 = var8.memberTypes();\n            LinkedHashMap var10 = new LinkedHashMap(var8.memberDefaults());\n            int var11 = var0.getShort() & '\\uffff';\n\n            for(int var12 = 0; var12 < var11; ++var12) {\n                int var13 = var0.getShort() & '\\uffff';\n                String var14 = var1.getUTF8At(var13);\n                Class var15 = (Class)var9.get(var14);\n                if (var15 == null) {\n                    skipMemberValue(var0);\n                } else {\n                    Object var16 = parseMemberValue(var15, var0, var1, var2);\n                    if (var16 instanceof AnnotationTypeMismatchExceptionProxy) {\n                        ((AnnotationTypeMismatchExceptionProxy)var16).setMember((Method)var8.members().get(var14));\n                    }\n\n                    var10.put(var14, var16);\n                }\n            }\n\n            return annotationForMap(var6, var10);\n        }\n    }\n````\n\n\n\ncretateAnnotationData()函数：\ndeclaredAnnnotations用来存储所有的注解\n\n![image-20210721093838070](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210721093838070.png)\n\nclass和ClassLoader\n","tags":["java基础"],"categories":["java"]},{"title":"Hbase学习记录","url":"/2022/01/29/Hbase学习记录/","content":"## 1.hbase的逻辑存储结构\n\n![img](https://img-blog.csdnimg.cn/20200324193055481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dvZGxvdmVkYW5pZWw=,size_16,color_FFFFFF,t_70)\n<!--more-->\n* 1.每条数据都具有一个rowkey，其具有唯一性，相当于mysql表中的主键（primary key）。\n\n* 2.其次是有一个列簇，相当就是列的集合，一个列簇在对应物理存储上相当于一个文件夹，当一个列簇中的字段越来越多的时候，就需要对表进行纵向的切分，便有了列簇的概念。\n\n* 3.当表中的数据越来越多的时候，表就会变的越来越高，需要对表进行横向的切分，这个切片就是region。\n\n  hbase的表其实就是kv结构的，只不过这个v是个多维的结构。\n\n## 2.Hbase的物理存储结构\n\n![image-20210712142811471](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210712142811471.png)\n\n由上图可以看出，在Hbase中一条数据会被当成多条来存储。每一条Hbase存储的数据都有完整的row key，列簇，列，时间戳和操作类型。\n\n## 3.Hbase和mysql的对比\n\n![image-20210712143823927](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210712143823927.png)\n\n## 4.Hbase的优势\n\n* 1.横向扩展：当现有服务出现瓶颈不需要停止现有的集群来提升配置，只需要在现有运行的集群中添加行的机器节点即可，一旦新的RegionServer建立完成，集群会开始重新调整。\n* 2.列式存储：Hbase是面向来存储的，\n* 每个列都单独存储，所以在Hbase中列是连续存储的，行不是。列可以动态的进行添加，列为空时就不会进行存储，节省空间。\n* 3.半结构化或者非结构化数据：Hbase支持半结构化或者非结构化的数据存储，因为对于数据字段不够确定或者杂乱无章非常难按一个概念去进行抽取的数据非常适合。\n* 4.高并发，随机读写：Hbase采用LSMT架构（log structured merge tree）进行设计，会周期性地将小文件合并成为大文件以减少磁盘访问同时减少NameNode压力。解决了HDFS不能随机读写的问题，在写入数据方面有很大的优势，特别是在pb级数据上面明显。\n* 5.自动切分：Hbase的数据是存在多个Region Server中的Region组成的，这些RegionServer又分布在不同的dataNode上面，如果一个region增长到一个阈值，为了减少负载均衡和减少IO，HBase会自动的或者手动干预将该Region切分成更小的region,也称之为subRegion。\n* 6.自动故障处理和负载均衡：Hbase是运行在HDFS之上的，Hbase的数据都是以多副本的方式来存放，数据也是分布式存放，数据的恢复可以得到保障。\n\n## 5.Hbase的基本架构和原理\n\n<img src=\"https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210712153141418.png\" alt=\"image-20210712153141418\"  />\n\n* 5.1Region Server（RS）\n\n  是Region的管理者，其实现类是HRegionServer，主要作用：\n\n  * 对数据的操作get,put,delete\n\n  * 对region的操作，splitRegion，compactRegion\n\n    Rs相当实现是在关系型数据库中DML的操作，实现的是数据的管理。\n\n* 5.2Master\n\n  Master时所有RS的管理者，其实类是HMaster,主要作用：\n\n  * 对于表的操作create，delete,alter\n  * 对于region Server的操作，分配region到每一个RS,监控每一个RS的状态，负载均衡和故障转移，发现失效的RS重新为他们分配自己负责的region。\n  * hdfs上面的垃圾回收，标记为删除而且经过major compact的文件。\n\n* 5.3zookeeper\n\n  Hbase通过zookeeper来做master的高可用，Rs的监控元数据的入口以及集群配置的维护工作。\n\n  * 负责Master高可用是切换工作。\n  * 降低Master的负担，承担Master的部分工作。\n\n* 5.4HDFS\n\n  HDFS为Hbase提供最底层的数据存储服务，同时为Hbase提供高可用的支持，底层的最终存储是构建在HDFS之上的。\n\n## 6.Hbase的核心组件\n\n![image-20210712154525588](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210712154525588.png)\n\n一个Rs是由一个HLog（默认是一个，1.1版本可以开启多个HLog），一个BlockCache以及多个Store组成。\n\n* Hlog是用来保证数据写入的可靠性，\n* BlockCache是可以将数据缓存到内存中以提升数据读取性能；\n* Region是HBase中数据表的一个数据分片，每个表中可以有多个分区，对数据的横向切分。同时一个RS上面通常会负责多个region的数据读写。一个region通常包含多个Store。\n* store存放的是对应列簇的数据，如一个表中有有两个列簇，那么在每个region中就会有两个store。每个store中有且仅有一个MemStore和多个HFile。用户数据写入时会将对应的列簇信息写入到Memstre中，同时一旦写入的数据超过了设定的阈值，系统会将Memstore中的数据落盘形成HFile文件。HFile存放在HDFS之上，是一种定制化的数据存储文件，方便用户进行数据读取。其结构关系如下图所示。\n\n![img](https://img-blog.csdnimg.cn/20201123232011218.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2dvZGxvdmVkYW5pZWw=,size_16,color_FFFFFF,t_70)\n\n* storeFile 保存数据的物理文件，StoreFile以HFile的形式存储到HDFS上，每个Store会有一个或者多个StoreFile(HFile),数据在每个StroreFile中是有序的，HFile可以理解为一种文件格式，像doc,txt等一样的概念，之不是是以key，value的形式存储。\n* MemStore写缓存，由于HFile中数据要求是有序的，所以数据是先存到MemStore中，排好序后，等到达刷写时才会刷写到Hfile中，每次刷写都会形成一个新的Hfile。\n* HLog由于数据要经过mem store排序后才会写到hfile中，但把数据保存到内存中会有很高的概率导致数据丢失，为了解决这个问题，数据在写入MemStored的同时，也会写在一个叫做write-ahead-logfile的文件中，当系统出现问题时，数据就可以通过这个日志文件来重建。wal类似于msyql的binlog，用来做灾难恢复，Hlog记录所有数据的变更，一旦数据修改，就可以从Hlog恢复，每个HRegionServer维护一个Hlog。\n* HRegion 就是所说的region，一个tabel由一个或者多个region组成，一个region可以看成table按照行切分且有序的数据块，每个region都有自身的StartKey，EndKey。一个region由一个store或者多个store来组成，每个store中都相当于一个列簇，相同列簇中的列都保存在一个store中，同一个tabel的region会分布到集群中不同的RS中以实现读写请求的负载均衡，所以一个RS中会包含来自不同table中的N个region。\n\n## 7.Hbased的写入数据过程\n\n![image-20210712170751610](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210712170751610.png)\n\n**写入流程：**写流程：\n1）Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server。\n2）访问对应的 Region Server，获取 hbase:meta 表，（同时meta table是不会被split的）根据读请求的 namespace:table/rowkey，查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问。\n3）与目标 Region Server 进行通讯；\n4）将数分别写入HLog和MemStore中，数据会在MemStore 进行排序（若MemStore中数据有丢失，则可以从HLog中进行恢复）；\n5）向客户端发送 ack；（当写入到内存时就可立即返回，代表写入成功，这也是Hbase，I/O高性能的保证）\n6）当MemStore 的数据达到一定阈值后，将数据刷写到StoreFile文件。\n7）当多个StoreFile文件达到一定大小后会触发Compact操作，合并为一个StoreFile,这里同时进行版本的合并（更新）及数据的删除\n8）当Compact后会逐步形成越来越大的StoreFile,此时会触发Split操作，把当前的StoreFile进行分割，相当于一个大的Region进行分割的过程，大的Region分割成两个。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["hbase"],"categories":["bigData","hbase"]},{"title":"Hive学习记录","url":"/2022/01/29/Hive学习记录/","content":"\n##  1.概述\n\nHive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转化成Mapreduce任务进行，可以通过类sql的语句快速地实现MapReduce统计，不必开发专门的MapReduce应用，Hive是一个数据仓库基础工具在Hadoop中用来处理结构化数据，在hadoop之上，总归为大数据。<!--more-->Hive的表其实就是HDFS的目录/文件，按照表名把文件夹分开，如果是分区表，那么分区值就是子文件夹，可以直接砸MP job里使用这些数据。\n\n![image-Hive系统架构图](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210714150417001.png)\n\n## 2.Hive系统架构\n\nCLI，即为shell命令行\n\nJDBC、ODBC 是Hive的java，与JDBC相似\n\nWebGui是通过浏览器访问hive。\n\nMetastore组件：hive将元数据存储在metaStore中，目前只支持mysql，derby，Hive中的元数据包括表的名字，表的列和分区以及属性，表的属性（是否为外部表），表的数据所在的目录。同时Metastore主要由两部分组成：Metastore服务和后台数据的存储。后台数据存储的介质就是关系数据库，例如hive默认的嵌入式磁盘数据库derby，还有mysql数据库。metastore服务是建立在后台数据存储介质之上的，并且可以和hvie服务进行交互的服务组件，默认情况下metaStore服务是和hive服务安装在一起的，运行在同一个进程中。但是也可以让metaStore服务独立安装在一个集群中，hive远程调用metaStore服务。\n\n解释器，编译器，优化器完成HQL查询语句从词法分析，语法分析，编译，优化以及查询计划的生成。生成的查询计划存储在HDFS中，并随后有MapReduce来调用执行。\n\n![hive使用过程](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210714153558531.png)\n\n相当于用户写的sql经过hive转换成mapreduce，再调用hadoop中的mp，去调用hdfs中的数据进行查询任务。\n\n## 3.和传统数据库的对比\n\n|              | Hive                  | ReBMS                  |\n| ------------ | --------------------- | ---------------------- |\n| 查询语言     | HQL                   | SQL                    |\n| 数据存储     | HDFS                  | Raw Device or local FS |\n| 执行         | MapReduce             | Excutor                |\n| 执行延迟     | 大                    | 小                     |\n| 处理数据规模 | 大                    | 小                     |\n| 索引         | 0.8之后加入了位图索引 | 有复杂的索引           |\n\nhive中的所有数据都是存储在HDSF中，没有专门的数据存储格式，可以支持Text，Sequencefile,ParquetFile,RCFILE等。需要在建表的时候配置hive数据中的列分隔符和行分隔符，hive就可以解析数据。Hive中的数据模型主要包括：DB，Tabel，External Table，Partition，Bucket。db是在hdfs中表现为${hivem.metastore.warehourse.dir}目录下一个文件夹。table是db目下一个文件夹。external table外部表和table类似，不过其数据存放位置可以在任意指定路径。普通表删除表后，hdfs上的文件就删除了，外部表删除后，hdfs上的文件没有删除，之事把文件删除了。partition在hdfs中表现为在table目录下的子目录。bucket桶在hdfs中表现为同一个表目录下根据hash散列之后的多个文件，会根据不同的文件把数据放在不同的文件中。hive表的数据是分为两部分的一部分是存放在hdfs上面的data，一部分是存放在mysql上面的data。\n\n","tags":["bigData","hive"],"categories":["bigData","hive"]},{"title":"Kudu学习记录","url":"/2022/01/29/Kudu学习记录/","content":"\n![image-20210723145108116](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210723145108116.png)\n<!--more-->\n## 1.概述\n\n如上所示是kuda的基本架构，kuda也是典型的主从架构，一个kuda集群有主节点（master）和若干个从节点（table server）组成。master负责管理集群的元数据（类似hbase master），tablet server负责数据存储（类似hbase regionserver）。在生产环境中，一般会部署多个master实现高可用（奇数个，一般为三个），tabletserver一般也是奇数个。\n\n## 2.kudu的基本术语\n\n**table（表）**\n\ntable和其他关系型数据库的一样。table是数据存储在kudu的位置，table会被宰水平分成很多段，每个段称为tablet。\n\n**tablet**\n\n一个tablet是一张表中连续的segment（片段），类似于hbase中的region，或者是关系型数据库中的partition分区。每一个tablet都存储着一定连续range的数据，同时一张表上的两个tablet上的range不会重叠，一张表的所有tablet会包含这个表的所有range；同时在分布式系统上，kudu的tablet也是有冗余设置，同一个tablet会存储多份，存放在多个不同的tablet server上面，并且在任何一个给定的时间点，其中只有一个tablet server是leader，其他的都是follwer tablet，每个tablet都可以提供读请求，但是只有leader可以负责写请求。\n\n**tabletserver**\n\ntabletserver是kudu集群中的从节点，负责数据存储，并提供数据读写服务，一个tablet server存储了table表中的tablet，同时一个tabletserver中存储了多个不同的tablet，同一个tablet的不同备份也会存在与不同的tabletserver上面，当一个tablet在该tabletserver上市leager，则其他tabletserver从当该tablet的follower副本。\n\n**master**\n\n集群的主节点，负责集群管理，元数据管理等功能，保持跟踪所有的tablets，tabletservers,catalog tagbles(目录表)和其他与集群相关的meta data。同样的在同一个时间也只有一个起作用的master（也就是leader），如果当前的leader消失了就选举一个新的leader，会通过raft协议来进行选举（todo：后面来看raft协议）。\n\n**raft consensus algorithm**\n\nkudu 使用 Raft consensus algorithm 作为确保常规 tablet 和 master 数据的容错性和一致性的手段。通过 Raft协议，tablet 的多个副本选举出 leader，它负责接受请求和复制数据写入到其他follower副本。一旦写入的数据在大多数副本中持久化后，就会向客户确认。给定的一组N副本（通常为 3 或 5 个）能够接受最多(N - 1)/2 错误的副本的写入。\n\n**Catalog Table（目录表）**\n\ncatalog table是kudu是元数据表，它存储有关tables和tablets的信息.catalog table（目录表）不能被直接读写，它只能通过客户端 API中公开的元数据操作访问。\ncatalog table存储以下两类元数据：\n1、tables：table schemas 表结构，locations 位置，states 状态\n2、tablets：现有tablet 的列表，每个 tablet 的副本所在哪些tablet server，tablet的当前状态以及开始和结束的keys（键）。\n\n## 3.语法\n\n使用impala创建表：必须首先列出构成主键的列\n\n```sql\nCREATE TABLE my_first_table\n(\n  id BIGINT,\n  name STRING,\n  PRIMARY KEY(id)\n)\nPARTITION BY HASH PARTITIONS 16  - 分tablet\nSTORED AS KUDU\nTBLPROPERTIES (\n  //kudu的集群地址\n  'kudu.master_addresses' = 'hadoop01:7051,hadoop02:7051,hadoop03:7051', \n  //kudu中表的名字\n    'kudu.table_name' = 'my_first_table'\n);\n```\n\n","tags":["bigData","kudu"],"categories":["bigData","kudu"]},{"title":"Spark基础学习","url":"/2022/01/29/Spark基础学习/","content":"\n## 1.Spark概述\n\nSpark是一种基于内存的快速，通用可扩展的大数据分析引擎。\n<!--more-->\n发展历史：\n\n![image-20210713141438921](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210713141438921.png)\n\nSpark特点：\n\n1）、**运行速度快**：与Hadoop的MR相比，Spark基于内存的运算要快100倍以上，基于硬盘的计算也快10倍以上。使用DAG（有向无环图）执行引擎以支持循环数据流与内存计算。计算结果存放于内存中，\n\n2）、**易用性好**：支持使用Scala、java、Python和R语言进行编程，可以通过Spark shell进行交互式编程。\n\n3）、**通用性强**：spark提供了完整而又强大的的工具，包括sql查询，流式计算，机器学习和图算法组件等等\n\n4）、**兼容性强**：可运行与独立的集群环境中，可运行与Hadoop中，也可运行在Amazon EC2等云环境中。并且可以访问HDFS，Hbase，hive等多种数据源。\n\nSpark主要的模块：\n\n![image-20210713141601479](https://bevishe.oss-cn-hangzhou.aliyuncs.com/img/image-20210713141601479.png)\n\nspark core：实现spark的基本功能，包含任务调度，内存管理，错误恢复，与存储系统交互等模块。同时其中还包含了对RDD（弹性分布式数据集）的API定义。\n\nSpark SQL：是Spark用来操作结构化数据的程序包，通过使用Spark SQL，可以使用SQL或者HQL来查询数据，支持多种数据来源如：Hive表，Parque以及JSON等。\n\nSpark Streaming：是Spark提供对实时数据进行流式计算的组件。包括用来操作数据流的API，并且与Spark core中的RDD API高度对应。\n\nSpark MLib：提供常见的机器学习（ML）功能的程序库。包含分类，回归，聚类，协同过滤，还提供了模型评估，数据导入等功能。\n\n集群管理器：Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算，为了实现这样的需求，同时获得最大的灵活性，Spark支持在各种集群管理器（Cluster Manager）上运行，包括Hadoop YARN，Apache Mesos,以及Spark自带的调度器，叫做独立调度器。\n","tags":["spark"],"categories":["bigData","spark"]},{"title":"parquest学习记录","url":"/2022/01/29/parquest学习记录/","content":"\nparquest是一种列式存储，只是一种存储格式，与上层平台，语言无关，当前已经适配的组件包括：\n<!--more-->\n- 询引擎：Hive\\Impala\\Pig\\Presto\\Drill\\Tajo\\HAWQ\\IBM Big SQL\n- 计算引擎：MapReduce\\Spark\\Cascading\\Crunch\\Scalding\\Kite\n- 数据模型：Avro\\Thrift\\Protocol Buffers\n\n数据可能是存储在hive或者是hbase上面，底层可能都是hdfs上面\n","tags":["kudu"],"categories":["bigData","kudu"]}]